{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time, random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.scalar_summary('stddev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=\"weight\")\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=\"bias\")\n",
    "\n",
    "def max_pool_2x2(x, name = \"\"):\n",
    "    name = name if name != \"\" else 'max_pool_' + str(random.randint(1, 1 << 30))\n",
    "    with tf.name_scope(name):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = \"activation\")\n",
    "\n",
    "def conv_layer(prev_layer, n_output, filter_width, activation = tf.identity, stride = 1, name = \"\"):\n",
    "    name = name if name != \"\" else 'conv_' + str(random.randint(1, 1 << 30))\n",
    "    \n",
    "    width_prev_layer = int(prev_layer.get_shape()[3])\n",
    "    with tf.name_scope(name):\n",
    "        Weight_mat = weight_variable([filter_width, filter_width, width_prev_layer, n_output])\n",
    "        variable_summaries(Weight_mat, name+'/weights')\n",
    "        res = tf.nn.conv2d(\n",
    "            prev_layer, \n",
    "            Weight_mat, \n",
    "            strides=[1, stride, stride, 1], \n",
    "            padding='SAME') + bias_variable([n_output])\n",
    "        return activation(res)\n",
    "\n",
    "def full_connect_layer(prev_layer, n_output, activation, name = \"\"):\n",
    "    \"\"\"\n",
    "    transform the previous layer to a \"flat\" matrix\n",
    "    And then fully connect to the next layer\n",
    "\n",
    "    input:\n",
    "    prev_layer: The previous layer\n",
    "    activation: ghd activation function of the nervous\n",
    "    K: size of the output layer \n",
    "\n",
    "    ouput:\n",
    "    a \"flat\" matrix with size K\n",
    "    \"\"\"\n",
    "    name = name if name != \"\" else 'fc_' + str(random.randint(1, 1 << 30))\n",
    "    with tf.name_scope(name):\n",
    "        sz_prev = int(reduce(lambda x,y: x*y, prev_layer.get_shape()[1:]))\n",
    "        flat = tf.reshape(prev_layer, [-1, sz_prev])\n",
    "        Weight = weight_variable([sz_prev, n_output])\n",
    "        Bias = bias_variable([n_output])\n",
    "        return activation(tf.matmul(flat, Weight) + Bias, name = \"activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 224, 224, 3], name=\"x-input\")\n",
    "sz_y = 17\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, sz_y], name=\"y-input\")\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "network = conv_layer(x, 64, 3, tf.nn.relu)\n",
    "network = conv_layer(network, 64, 3, tf.nn.relu)\n",
    "network = max_pool_2x2(network)\n",
    "\n",
    "network = conv_layer(network, 128, 3, tf.nn.relu)\n",
    "network = conv_layer(network, 128, 3, tf.nn.relu)\n",
    "network = max_pool_2x2(network)\n",
    "\n",
    "network = conv_layer(network, 256, 3, tf.nn.relu)\n",
    "network = conv_layer(network, 256, 3, tf.nn.relu)\n",
    "network = conv_layer(network, 256, 3, tf.nn.relu)\n",
    "network = max_pool_2x2(network)\n",
    "\n",
    "network = conv_layer(network, 512, 3, tf.nn.relu)\n",
    "network = conv_layer(network, 512, 3, tf.nn.relu)\n",
    "network = conv_layer(network, 512, 3, tf.nn.relu)\n",
    "network = max_pool_2x2(network)\n",
    "\n",
    "network = conv_layer(network, 512, 3, tf.nn.relu)\n",
    "network = conv_layer(network, 512, 3, tf.nn.relu)\n",
    "network = conv_layer(network, 512, 3, tf.nn.relu)\n",
    "network = max_pool_2x2(network)\n",
    "\n",
    "network = full_connect_layer(network, 4096, tf.nn.relu)\n",
    "network = tf.nn.dropout(network, keep_prob)\n",
    "network = full_connect_layer(network, 4096, tf.nn.relu)\n",
    "network = tf.nn.dropout(network, keep_prob)\n",
    "network = full_connect_layer(network, sz_y, tf.nn.softmax)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(network), reduction_indices=[1]))\n",
    "train_step = tf.train.RMSPropOptimizer(1e-3).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(network, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import oxflower17\n",
    "\n",
    "class NP_Dataset(object):\n",
    "    def __init__(self, pX, pY):\n",
    "        self._X = pX\n",
    "        self._Y = pY\n",
    "        assert np.shape(self._X)[0] == np.shape(self._Y)[0]\n",
    "        self._n_sample = np.shape(self._X)[0]\n",
    "        self._index_in_epoch = 0\n",
    "        self._epoch_completed = 0\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._n_sample:\n",
    "            assert batch_size <= self._n_sample\n",
    "            self._epoch_completed += 1\n",
    "            # Shuffle\n",
    "            perm = np.arange(self._n_sample) \n",
    "            np.random.shuffle(perm)\n",
    "            self._X = self._X[perm]\n",
    "            self._Y = self._Y[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "        end = self._index_in_epoch\n",
    "        return self._X[start:end], self._Y[start:end]\n",
    "\n",
    "flower = NP_Dataset(*oxflower17.load_data())\n",
    "# Test\n",
    "# batch = flower.next_batch(32)\n",
    "# batch = flower.next_batch(1000)\n",
    "# batch = flower.next_batch(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rm this dir before run again\n",
    "# execute: tensorboard --logdir=./CNN_logs\n",
    "# summaries_dir = './VGG16_logs'\n",
    "# tf.scalar_summary('cross_entropy', cross_entropy)\n",
    "# tf.scalar_summary('accuracy', accuracy)\n",
    "# merged = tf.merge_all_summaries()\n",
    "# train_writer = tf.train.SummaryWriter(summaries_dir + '/train', sess.graph)\n",
    "# test_writer = tf.train.SummaryWriter(summaries_dir + '/test')\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "start_train_time = time.time()\n",
    "for i in range(2000):\n",
    "    batch = flower.next_batch(32)\n",
    "    print(\"i = %d\", i)\n",
    "    if i%100 == 99:\n",
    "        # Train\n",
    "        # run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        # run_metadata = tf.RunMetadata()\n",
    "        # summary, train_accuracy = sess.run([merged, accuracy], feed_dict = {\n",
    "        #         x: batch[0], \n",
    "        #         y_: batch[1],\n",
    "        #         keep_prob: 1.0})\n",
    "        # train_writer.add_summary(summary, i)\n",
    "        # train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "        train_accuracy = accuracy.eval(feed_dict = {\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        # Test\n",
    "        # summary, test_accuracy = sess.run([merged, accuracy], feed_dict={\n",
    "        #     x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n",
    "        # test_writer.add_summary(summary, i)\n",
    "        # print(\"test accuracy %g\"%test_accuracy)\n",
    "    else:  # Record a summary\n",
    "        # summary, _ = sess.run([merged, train_step], feed_dict={\n",
    "        #         x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "        # train_writer.add_summary(summary, i)\n",
    "        train_step.run(feed_dict={\n",
    "                x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "end_train_time = time.time()\n",
    "print(\"Total Training Time:\",(end_train_time - start_train_time))\n",
    "\n",
    "# train_writer.close()\n",
    "# test_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
